{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import jieba\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斷詞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用繁體jieba對各個feature斷詞\n",
    "- 移除某些停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/n7/kd4twb9x5_z9c2vf17wfmgvm0000gn/T/jieba.cache\n",
      "Loading model cost 0.608 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_excel('train.xlsx')\n",
    "test = pd.read_excel('test.xlsx', names=['user_id','company','product','depart','position','description','fix'])\n",
    "\n",
    "train_Y_indus = train['a08a01']\n",
    "train_Y_occup = train['a08a02']\n",
    "train_X = train[['k_a08a_1','k_a08a_2','k_a08a_3','k_a08a_4','k_a08a_5','fix']]\n",
    "train_X.columns = ['company','product','depart','position','description','fix']\n",
    "\n",
    "all_x=pd.concat((train_X,test[['company','product','depart','position','description','fix']]),axis=0)\n",
    "all_x=all_x.reset_index()\n",
    "\n",
    "jieba.load_userdict([\"化粧品\", \"買賣\", \"卷釘\", \"影印機\", \"耗財\", \"製造\", \"國中小\", \"製作\", \"紙箱\", \"褓姆\", \"放存款\", \"汽車\", \"快炒\", \"進出口\",\n",
    "           \"手機\", \"電腦\", \"生產\", \"塑膠\", \"飛機\", \"麵包\", \"被動\", \"元件\", \"半導體\", \"機械\", \"便當\", \"維修\", \"冷氣\", \"托育\", \"金屬\",\n",
    "           \"才藝\", \"軟體\", \"產品\", \"排汗\", \"月子\", \"美髮\", \"連接器\", \"鋼鐵\", \"蓮藕\", \"產品\", \"晶圓\", \"代工\", \"五穀\", \"敎学\", \"賣麵\",\n",
    "           \"不知道\", \"打掃\", \"魚塭\", \"餐飲\", \"批發\", \"販售\", \"捲餅\", \"國中\", \"國小\", \" 高中職\", \"傢俱\", \"通訊\", \"教育\", \"後勤\",\n",
    "           \"隨身碟\", \"鮪魚\", \"不鏽鋼\", \"鐵窗\", \"敎育\", \"髮型\", \"连花\", \"網路\",\"7-11\",\"一般\",\"統一\",\"第一\",\"中西式\",\"成會\",'塩酥雞'\n",
    "                    ,'噴藥','研究','輕鋼架'])\n",
    "\n",
    "for idx,item in all_x.iterrows():\n",
    "    seg_list = jieba.cut(item['description'])\n",
    "    all_x.loc[idx,'description seq']=\"/\".join(seg_list)\n",
    "    seg_list2 = jieba.cut(item['product'])\n",
    "    all_x.loc[idx,'product seq']=\"/\".join(seg_list2)\n",
    "    seg_list3 = jieba.cut(item['company'])\n",
    "    all_x.loc[idx,'company seq']=\"/\".join(seg_list3)\n",
    "    seg_list4 = jieba.cut(item['position'])\n",
    "    all_x.loc[idx,'position seq']=\"/\".join(seg_list4)\n",
    "    seg_list4 = jieba.cut(item['depart'])\n",
    "    all_x.loc[idx,'depart seq']=\"/\".join(seg_list4)\n",
    "    \n",
    "all_x['indus seq']=all_x['company seq']+'/'+all_x['product seq']+'/'+all_x['description seq']+'/'+all_x['position']\n",
    "all_x['occup seq']=all_x['depart']+'/'+all_x['position seq']+'/'+all_x['company seq']+'/'+all_x['description seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopword(str_val):\n",
    "    stopWords=['丶', '(', ')', '.', '，','*','的','等','及','XX','。','大','小','與','、','和','做','在','之',\"跟\",\"給\",\"Xx\",'-',\"並\",\"其\",'一','上','是','等等','為','但','&&']\n",
    "    arr=str_val.split('/')\n",
    "    arr=list(map(lambda x: x.capitalize(),arr))\n",
    "    arr=list(filter(lambda a: a not in stopWords and a != '\\n' and a!=\" \" and a!='' and not a.isdigit(), arr))\n",
    "    arr=\"/\".join(arr)\n",
    "    return  arr\n",
    "\n",
    "all_x['indus seq']=all_x['indus seq'].apply(filter_stopword)\n",
    "all_x['occup seq']=all_x['occup seq'].apply(filter_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 將斷詞結果轉換成tfidf\n",
    "- 將y轉成one hot encoding形式\n",
    "- 使用nn模型進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/michelle/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/michelle/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               3584512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 3,880,000\n",
      "Trainable params: 3,880,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/michelle/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "3200/3200 [==============================] - 4s 1ms/step - loss: 3.8604 - acc: 0.0909\n",
      "Epoch 2/20\n",
      "3200/3200 [==============================] - 4s 1ms/step - loss: 3.2537 - acc: 0.2209\n",
      "Epoch 3/20\n",
      "3200/3200 [==============================] - 3s 1ms/step - loss: 2.6614 - acc: 0.3841\n",
      "Epoch 4/20\n",
      "3200/3200 [==============================] - 3s 951us/step - loss: 2.1305 - acc: 0.5031\n",
      "Epoch 5/20\n",
      "3200/3200 [==============================] - 4s 1ms/step - loss: 1.6990 - acc: 0.5988\n",
      "Epoch 6/20\n",
      "3200/3200 [==============================] - 3s 837us/step - loss: 1.3588 - acc: 0.6638\n",
      "Epoch 7/20\n",
      "3200/3200 [==============================] - 3s 815us/step - loss: 1.1031 - acc: 0.7238\n",
      "Epoch 8/20\n",
      "3200/3200 [==============================] - 4s 1ms/step - loss: 0.8848 - acc: 0.7803\n",
      "Epoch 9/20\n",
      "3200/3200 [==============================] - 3s 835us/step - loss: 0.7165 - acc: 0.8178\n",
      "Epoch 10/20\n",
      "3200/3200 [==============================] - 3s 889us/step - loss: 0.5720 - acc: 0.8531\n",
      "Epoch 11/20\n",
      "3200/3200 [==============================] - 3s 811us/step - loss: 0.4825 - acc: 0.8694\n",
      "Epoch 12/20\n",
      "3200/3200 [==============================] - 3s 823us/step - loss: 0.3888 - acc: 0.8984\n",
      "Epoch 13/20\n",
      "3200/3200 [==============================] - 2s 777us/step - loss: 0.3408 - acc: 0.9075\n",
      "Epoch 14/20\n",
      "3200/3200 [==============================] - 3s 791us/step - loss: 0.2779 - acc: 0.9256\n",
      "Epoch 15/20\n",
      "3200/3200 [==============================] - 3s 782us/step - loss: 0.2325 - acc: 0.9344\n",
      "Epoch 16/20\n",
      "3200/3200 [==============================] - 3s 827us/step - loss: 0.2300 - acc: 0.9391\n",
      "Epoch 17/20\n",
      "3200/3200 [==============================] - 3s 810us/step - loss: 0.1934 - acc: 0.9494\n",
      "Epoch 18/20\n",
      "3200/3200 [==============================] - 3s 784us/step - loss: 0.1635 - acc: 0.9584\n",
      "Epoch 19/20\n",
      "3200/3200 [==============================] - 2s 775us/step - loss: 0.1501 - acc: 0.9572\n",
      "Epoch 20/20\n",
      "3200/3200 [==============================] - 2s 775us/step - loss: 0.1314 - acc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2b496cc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels_indus = 64\n",
    "vocab_size_indus = 7000\n",
    "batch_size_indus = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size_indus)\n",
    "tokenizer.fit_on_texts(all_x['indus seq'])\n",
    "x_train_indus = tokenizer.texts_to_matrix(all_x['indus seq'].iloc[:3200,], mode='tfidf')\n",
    " \n",
    "encoder_indus = LabelBinarizer()\n",
    "encoder_indus.fit(train_Y_indus)\n",
    "y_train_indus = encoder_indus.transform(train_Y_indus)\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(512, input_shape=(vocab_size_indus,)))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(0.7))\n",
    "model1.add(Dense(512))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(0.7))\n",
    "model1.add(Dense(num_labels_indus))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model1.fit(x_train_indus,\n",
    "           y_train_indus,\n",
    "           batch_size=batch_size_indus,\n",
    "           epochs=20,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 職業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               3072512   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 119)               61047     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 119)               0         \n",
      "=================================================================\n",
      "Total params: 3,396,215\n",
      "Trainable params: 3,396,215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "3200/3200 [==============================] - 3s 866us/step - loss: 4.5581 - acc: 0.0522\n",
      "Epoch 2/15\n",
      "3200/3200 [==============================] - 2s 696us/step - loss: 3.9296 - acc: 0.1647\n",
      "Epoch 3/15\n",
      "3200/3200 [==============================] - 2s 692us/step - loss: 3.1734 - acc: 0.3444\n",
      "Epoch 4/15\n",
      "3200/3200 [==============================] - 2s 680us/step - loss: 2.3355 - acc: 0.5197\n",
      "Epoch 5/15\n",
      "3200/3200 [==============================] - 2s 684us/step - loss: 1.6638 - acc: 0.6369\n",
      "Epoch 6/15\n",
      "3200/3200 [==============================] - 2s 684us/step - loss: 1.1936 - acc: 0.7344\n",
      "Epoch 7/15\n",
      "3200/3200 [==============================] - 2s 681us/step - loss: 0.8308 - acc: 0.8184\n",
      "Epoch 8/15\n",
      "3200/3200 [==============================] - 2s 683us/step - loss: 0.5846 - acc: 0.8663\n",
      "Epoch 9/15\n",
      "3200/3200 [==============================] - 2s 678us/step - loss: 0.4209 - acc: 0.9037\n",
      "Epoch 10/15\n",
      "3200/3200 [==============================] - 2s 702us/step - loss: 0.3269 - acc: 0.9234\n",
      "Epoch 11/15\n",
      "3200/3200 [==============================] - 2s 685us/step - loss: 0.2702 - acc: 0.9406\n",
      "Epoch 12/15\n",
      "3200/3200 [==============================] - 2s 705us/step - loss: 0.1910 - acc: 0.9622\n",
      "Epoch 13/15\n",
      "3200/3200 [==============================] - 2s 687us/step - loss: 0.1825 - acc: 0.9544\n",
      "Epoch 14/15\n",
      "3200/3200 [==============================] - 2s 691us/step - loss: 0.1456 - acc: 0.9697\n",
      "Epoch 15/15\n",
      "3200/3200 [==============================] - 2s 708us/step - loss: 0.1280 - acc: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2b667f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels_occup = 119\n",
    "vocab_size_occup = 6000\n",
    "batch_size_occup = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size_occup)\n",
    "tokenizer.fit_on_texts(all_x['occup seq'])\n",
    "x_train_occup = tokenizer.texts_to_matrix(all_x['occup seq'].iloc[:3200,], mode='tfidf')\n",
    " \n",
    "encoder_occup = LabelBinarizer()\n",
    "encoder_occup.fit(train_Y_occup)\n",
    "y_train_occup = encoder_occup.transform(train_Y_occup)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(512, input_shape=(vocab_size_occup,)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.6))\n",
    "model2.add(Dense(512))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.6))\n",
    "model2.add(Dense(num_labels_occup))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model2.fit(x_train_occup, \n",
    "           y_train_occup,\n",
    "           batch_size=batch_size_occup,\n",
    "           epochs=15,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size_indus)\n",
    "tokenizer.fit_on_texts(all_x['indus seq'])\n",
    "x_test_indus = tokenizer.texts_to_matrix(all_x['indus seq'].iloc[3200:,], mode='tfidf')\n",
    "\n",
    "labels = encoder_indus.classes_\n",
    "ypredclf=[]\n",
    "y_pred = model1.predict(x_test_indus)\n",
    "for i in y_pred:\n",
    "    ypredclf.append(labels[np.argmax(i)])\n",
    "    \n",
    "    \n",
    "tokenizer = Tokenizer(num_words=vocab_size_occup)\n",
    "tokenizer.fit_on_texts(all_x['occup seq'])\n",
    "x_test_occup = tokenizer.texts_to_matrix(all_x['occup seq'].iloc[3200:,], mode='tfidf')\n",
    " \n",
    "labels = encoder_occup.classes_\n",
    "ypredclfp=[]\n",
    "y_pred2 = model2.predict(x_test_occup)\n",
    "for i in y_pred2:\n",
    "    ypredclfp.append(labels[np.argmax(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv file\n",
    "a08a01=pd.concat((test['user_id'],pd.Series(ypredclf)),axis=1)\n",
    "a08a02=pd.concat((test['user_id'],pd.Series(ypredclfp)),axis=1)\n",
    "a08a01['res']=a08a01['user_id'].astype(str)+\"_a08a01,\"+a08a01[0].astype(str)\n",
    "a08a02['res']=a08a02['user_id'].astype(str) + \"_a08a02,\" + a08a02[0].astype(str)\n",
    "\n",
    "with open('result_'+datetime.datetime.now().strftime('%m%d%H%M')+'.csv', 'w') as myfile:\n",
    "    myfile.write('x01,prediction\\n')\n",
    "    for i in pd.concat((a08a01,a08a02),axis=0)['res']:\n",
    "        myfile.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 嘗試不同的dropout分別是0.5 0.6 0.7 並將結果進行voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1=pd.read_csv('result_06182248_0.69986.csv')\n",
    "result2=pd.read_csv('result_06241351_0.69851.csv')\n",
    "result3=pd.read_csv('result_06262031.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=result1.merge(result2,on='x01').merge(result3,on='x01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,item in merged.iterrows():\n",
    "    if item[['prediction_x','prediction_y','prediction']].value_counts().shape[0]==3:\n",
    "        merged.loc[idx,'vote']=item['prediction_x']\n",
    "    else:\n",
    "        merged.loc[idx,'vote']=item[['prediction_x','prediction_y','prediction']].value_counts().idxmax()\n",
    "merged['vote']=merged['vote'].astype(int)\n",
    "merged.columns=['x01','pred1','pred2','pred3','prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[['x01','prediction']].to_csv('end2.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
