{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning HW3: Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "# from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Classification via Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explore the problem of identifying smartphone position through probabilistic generative models. Motion sensors in smartphones provide valuable information for researchers to understand its owners. An interesting (and more challenging) task is to identify human activities through the data recorded by motion sensors. For example, we want to know whether the smartphone owner is walking, running, or biking. In this homework problem, we are going to tackle a simpler problem. We want to know the static position of the smartphone. There are six possible positions:\n",
    " * Phoneonback: The phone is laying on the back of the phone with the screen pointing up (away from the ground).\n",
    " * Phoneonfront: The phone is laying on the back of the phone with the screen pointing towards the ground\n",
    " * Phoneonbottom: The phone is standing on the bottom of the screen, meaning the bottom is pointed towards the ground\n",
    " * Phoneontop: The phone is standing on the top of the screen, meaning the top is pointed towards the ground\n",
    " * Phoneonleft: The phone is laying on the left side of the screen.\n",
    " * Phoneonright: The phone is laying on the right side of the screen.\n",
    "\n",
    "The input data is the reading of the accelerometer (cf. https://en.wikipedia.org/wiki/Accelerometer) in the smartphone. We have a training dataset that contains about 28,500 data points for phones in each of the six positions. The following is some basic information of the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct mypgc class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mypgc:\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "\n",
    "        positions = y_train.unique()\n",
    "        self.positions = positions\n",
    "\n",
    "        data_dic = {}\n",
    "        for po in self.positions:\n",
    "\n",
    "            innder_dic = {}\n",
    "            \n",
    "            filter_data = x_train.loc[x_train['label'] == po]\n",
    "            po_data = filter_data.drop([\"label\"], axis = 1).values\n",
    "\n",
    "            #the mean vector\n",
    "            mean = po_data.mean(axis=0)\n",
    "            innder_dic[\"mu\"] = mean\n",
    "\n",
    "            #the covariance matrix\n",
    "            cov = np.cov(po_data.T)\n",
    "            innder_dic[\"cov\"] = cov\n",
    "\n",
    "            #the inverse of covariance matrix\n",
    "            inverse = np.linalg.inv(cov)\n",
    "            innder_dic[\"prec\"] = inverse\n",
    "\n",
    "            #logarithm of the determinant of the covariance matrix\n",
    "            detcov = np.linalg.slogdet(cov)\n",
    "            innder_dic[\"detcov\"] = detcov\n",
    "\n",
    "            #number of observations for this label\n",
    "            n = len(po_data)\n",
    "            innder_dic[\"n\"] = n\n",
    "\n",
    "            #learned prior probability of this label\n",
    "            prior = n/len(y_train)\n",
    "            innder_dic[\"prior\"] = prior\n",
    "\n",
    "            data_dic[po] = innder_dic\n",
    "            self.trained_model = data_dic\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            \n",
    "            posteriors = []\n",
    "            for p in self.positions:\n",
    "                \n",
    "                likelihood = multivariate_normal.pdf(x, mean = self.trained_model[p][\"mu\"],\n",
    "                                                     cov = self.trained_model[p][\"cov\"])\n",
    "                prior_p = self.trained_model[p][\"prior\"]\n",
    "                posterior = likelihood * prior_p\n",
    "                posteriors.append(posterior)\n",
    "            \n",
    "            max_index = posteriors.index(max(posteriors))\n",
    "            pre = self.positions[max_index]\n",
    "            predictions.append(pre)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few rows are:\n",
      "\n",
      "Summary statistics:\n",
      "                   x              y              z\n",
      "count  167097.000000  167097.000000  167097.000000\n",
      "mean        0.357340       0.146807      -0.015550\n",
      "std         5.622396       5.552010       5.737150\n",
      "min        -9.770279      -9.966507      -9.908417\n",
      "25%         0.016617      -0.074875      -0.221497\n",
      "50%         0.142776       0.009628       0.025223\n",
      "75%         0.249893       0.295715       0.151032\n",
      "max        10.073685       9.980255      10.031113\n",
      "\n",
      "Label counts:\n",
      "Phoneonleft      29522\n",
      "Phoneonfront     29079\n",
      "Phoneonback      28566\n",
      "Phoneonbottom    27842\n",
      "Phoneontop       26401\n",
      "Phoneonright     25687\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open('phone_train.pickle', 'rb') as fh1:\n",
    "    traindata = pickle.load(fh1)\n",
    "    \n",
    "print(\"The first few rows are:\")\n",
    "traindata.head()\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(traindata.describe())\n",
    "print(\"\\nLabel counts:\")\n",
    "print(traindata['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = traindata['label']\n",
    "X_train = traindata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few rows are:\n",
      "          x         y         z          label\n",
      "0 -0.213196  0.031982 -9.370026   Phoneonfront\n",
      "1 -0.151306  9.761749  0.188354  Phoneonbottom\n",
      "2 -0.122742  9.771271  0.277618  Phoneonbottom\n",
      "3 -0.200104  9.805786  0.235962  Phoneonbottom\n",
      "4 -0.244141  0.039124 -9.412872   Phoneonfront\n",
      "\n",
      "Summary statistics:\n",
      "                  x             y             z\n",
      "count  83511.000000  83511.000000  83511.000000\n",
      "mean       0.283334      0.133812      0.363750\n",
      "std        5.612133      5.551263      5.703844\n",
      "min       -9.979858     -9.890595     -9.534271\n",
      "25%       -0.217957     -0.053711      0.193115\n",
      "50%        0.168472      0.049835      0.305191\n",
      "75%        0.375565      0.220779      0.584885\n",
      "max       10.068436      9.888657     10.371780\n",
      "\n",
      "Label counts:\n",
      "Phoneonleft      14779\n",
      "Phoneonfront     14517\n",
      "Phoneonback      14306\n",
      "Phoneonbottom    13887\n",
      "Phoneontop       13183\n",
      "Phoneonright     12839\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open('phone_test1.pickle', 'rb') as fh2:\n",
    "    test_data = pickle.load(fh2)\n",
    "\n",
    "print(\"The first few rows are:\")\n",
    "print(test_data.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(test_data.describe())\n",
    "print(\"\\nLabel counts:\")\n",
    "print(test_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = test_data['label']\n",
    "X_test = test_data.drop([\"label\"], axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate mypgc object with data and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgc1 = mypgc()\n",
    "pgc1.fit(X_train, Y_train)\n",
    "test_1 = X_test\n",
    "ypred = pgc1.predict(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction/correct label\n",
      "------------------------\n",
      "Phoneonfront / Phoneonfront\n",
      "Phoneonbottom / Phoneonbottom\n",
      "Phoneonbottom / Phoneonbottom\n",
      "Phoneonbottom / Phoneonbottom\n",
      "Phoneonfront / Phoneonfront\n",
      "Phoneonright / Phoneonright\n",
      "Phoneonfront / Phoneonfront\n",
      "Phoneontop / Phoneontop\n",
      "Phoneonfront / Phoneonfront\n",
      "Phoneonfront / Phoneonfront\n",
      "Phoneonfront / Phoneonfront\n",
      "Phoneonback / Phoneonback\n",
      "Phoneonleft / Phoneonleft\n",
      "Phoneonback / Phoneonback\n",
      "Phoneonbottom / Phoneonbottom\n",
      "Phoneonback / Phoneonback\n",
      "Phoneonright / Phoneonright\n",
      "Phoneonright / Phoneonright\n",
      "Phoneontop / Phoneontop\n",
      "Phoneonleft / Phoneonleft\n",
      "------------------------\n",
      "accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "correctness = 0\n",
    "\n",
    "print(\"prediction/correct label\")\n",
    "print(\"------------------------\")\n",
    "for i in range(len(Y_test)):\n",
    "    \n",
    "    if i < 20:\n",
    "        print(ypred[i], \"/\", Y_test[i])\n",
    "    if ypred[i] == Y_test[i]:\n",
    "        correctness += 1\n",
    "accuracy = correctness/len(Y_test)\n",
    "print(\"------------------------\")\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Logistic Regression with L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use to \"Adult\" dataset on the UCI machine learning reposition https://archive.ics.uci.edu/ml/datasets/Adult. The goal is to predict the label values of the income column, which can be either '>50K' or '<=50K.' The dataset had splitted the training and test data, and we are going to respect this particular train-test split in model testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Data preparation and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "train = pandas.read_csv(\"adult_train.csv\") \n",
    "test = pandas.read_csv(\"adult_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 categorical columns : ['workclass', 'race', 'income', 'education', 'sex', 'native-country', 'occupation', 'marital-status', 'relationship']\n",
      "There are 6 numerical columns : ['capital-gain', 'hours-per-week', 'age', 'fnlwgt', 'capital-loss', 'education-num']\n",
      "--- Remove rows if have missing values in column workclass\n",
      "--- Remove rows if have missing values in column race\n",
      "--- Remove rows if have missing values in column income\n",
      "--- Remove rows if have missing values in column education\n",
      "--- Remove rows if have missing values in column sex\n",
      "--- Remove rows if have missing values in column native-country\n",
      "--- Remove rows if have missing values in column occupation\n",
      "--- Remove rows if have missing values in column marital-status\n",
      "--- Remove rows if have missing values in column relationship\n",
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "#fix the extra \".\" in test data.\n",
    "test['income'].replace(regex=True,inplace=True,to_replace=r'\\.',value=r'')\n",
    "\n",
    "num_col = set(train.describe().columns)\n",
    "cat_col = set(train.columns) - num_col\n",
    "\n",
    "#want to have order\n",
    "num_col = list(num_col)\n",
    "cat_col = list(cat_col)\n",
    "\n",
    "print(\"There are %d categorical columns\" % len(cat_col), \":\", cat_col)\n",
    "print(\"There are %d numerical columns\" % len(num_col), \":\", num_col)\n",
    "\n",
    "show_details = 0\n",
    "if show_details > 0:\n",
    "    print(\"Check NA for numerical columns in training and test\")\n",
    "    print(\"... train\")\n",
    "    print(train[num_col].isna().any())\n",
    "    print(\"... test\")\n",
    "    print(test[num_col].isna().any())\n",
    "\n",
    "    print(\"Check NA for categorical columns\")\n",
    "    print(\"... train\")\n",
    "    print(train[cat_col].isna().any())\n",
    "    print(\"... test\")\n",
    "    print(test[cat_col].isna().any())\n",
    "\n",
    "    #there is no NA, but still need to handle the \"?\" problem.\n",
    "    for acol in cat_col:\n",
    "        print(\"---Unique values in \", acol)\n",
    "        print(train[acol].value_counts())\n",
    "\n",
    "#remove all rows with \"?\"\n",
    "for acol in cat_col:\n",
    "    print(\"--- Remove rows if have missing values in column\", acol)\n",
    "    ind1 = train[acol] != \"?\"\n",
    "    train = train[ind1]\n",
    "\n",
    "    ind2 = test[acol] != \"?\"\n",
    "    test = test[ind2]\n",
    "\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data summary statistics\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
      "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
      "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "count    32561.000000  \n",
      "mean        40.437456  \n",
      "std         12.347429  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n",
      "---Value count for workclass\n",
      " Private             22696\n",
      " Self-emp-not-inc     2541\n",
      " Local-gov            2093\n",
      " ?                    1836\n",
      " State-gov            1298\n",
      " Self-emp-inc         1116\n",
      " Federal-gov           960\n",
      " Without-pay            14\n",
      " Never-worked            7\n",
      "Name: workclass, dtype: int64\n",
      "---Value count for race\n",
      " White                 27816\n",
      " Black                  3124\n",
      " Asian-Pac-Islander     1039\n",
      " Amer-Indian-Eskimo      311\n",
      " Other                   271\n",
      "Name: race, dtype: int64\n",
      "---Value count for income\n",
      "<=50K    24720\n",
      ">50K      7841\n",
      "Name: income, dtype: int64\n",
      "---Value count for education\n",
      " HS-grad         10501\n",
      " Some-college     7291\n",
      " Bachelors        5355\n",
      " Masters          1723\n",
      " Assoc-voc        1382\n",
      "11th              1175\n",
      " Assoc-acdm       1067\n",
      "10th               933\n",
      "7th-8th            646\n",
      " Prof-school       576\n",
      "9th                514\n",
      "12th               433\n",
      " Doctorate         413\n",
      "5th-6th            333\n",
      "1st-4th            168\n",
      " Preschool          51\n",
      "Name: education, dtype: int64\n",
      "---Value count for sex\n",
      " Male      21790\n",
      " Female    10771\n",
      "Name: sex, dtype: int64\n",
      "---Value count for native-country\n",
      " United-States                 29170\n",
      " Mexico                          643\n",
      " ?                               583\n",
      " Philippines                     198\n",
      " Germany                         137\n",
      " Canada                          121\n",
      " Puerto-Rico                     114\n",
      " El-Salvador                     106\n",
      " India                           100\n",
      " Cuba                             95\n",
      " England                          90\n",
      " Jamaica                          81\n",
      " South                            80\n",
      " China                            75\n",
      " Italy                            73\n",
      " Dominican-Republic               70\n",
      " Vietnam                          67\n",
      " Guatemala                        64\n",
      " Japan                            62\n",
      " Poland                           60\n",
      " Columbia                         59\n",
      " Taiwan                           51\n",
      " Haiti                            44\n",
      " Iran                             43\n",
      " Portugal                         37\n",
      " Nicaragua                        34\n",
      " Peru                             31\n",
      " Greece                           29\n",
      " France                           29\n",
      " Ecuador                          28\n",
      " Ireland                          24\n",
      " Hong                             20\n",
      " Trinadad&Tobago                  19\n",
      " Cambodia                         19\n",
      " Laos                             18\n",
      " Thailand                         18\n",
      " Yugoslavia                       16\n",
      " Outlying-US(Guam-USVI-etc)       14\n",
      " Hungary                          13\n",
      " Honduras                         13\n",
      " Scotland                         12\n",
      " Holand-Netherlands                1\n",
      "Name: native-country, dtype: int64\n",
      "---Value count for occupation\n",
      " Prof-specialty       4140\n",
      " Craft-repair         4099\n",
      " Exec-managerial      4066\n",
      " Adm-clerical         3770\n",
      " Sales                3650\n",
      " Other-service        3295\n",
      " Machine-op-inspct    2002\n",
      " ?                    1843\n",
      " Transport-moving     1597\n",
      " Handlers-cleaners    1370\n",
      " Farming-fishing       994\n",
      " Tech-support          928\n",
      " Protective-serv       649\n",
      " Priv-house-serv       149\n",
      " Armed-Forces            9\n",
      "Name: occupation, dtype: int64\n",
      "---Value count for marital-status\n",
      " Married-civ-spouse       14976\n",
      " Never-married            10683\n",
      " Divorced                  4443\n",
      " Separated                 1025\n",
      " Widowed                    993\n",
      " Married-spouse-absent      418\n",
      " Married-AF-spouse           23\n",
      "Name: marital-status, dtype: int64\n",
      "---Value count for relationship\n",
      " Husband           13193\n",
      " Not-in-family      8305\n",
      " Own-child          5068\n",
      " Unmarried          3446\n",
      " Wife               1568\n",
      " Other-relative      981\n",
      "Name: relationship, dtype: int64\n",
      "\n",
      "\n",
      " Test data summary statistics\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  16281.000000  1.628100e+04   16281.000000  16281.000000  16281.000000   \n",
      "mean      38.767459  1.894357e+05      10.072907   1081.905104     87.899269   \n",
      "std       13.849187  1.057149e+05       2.567545   7583.935968    403.105286   \n",
      "min       17.000000  1.349200e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.167360e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.778310e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.383840e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.490400e+06      16.000000  99999.000000   3770.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "count    16281.000000  \n",
      "mean        40.392236  \n",
      "std         12.479332  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n",
      "---Value count for workclass\n",
      " Private             11210\n",
      " Self-emp-not-inc     1321\n",
      " Local-gov            1043\n",
      " ?                     963\n",
      " State-gov             683\n",
      " Self-emp-inc          579\n",
      " Federal-gov           472\n",
      " Without-pay             7\n",
      " Never-worked            3\n",
      "Name: workclass, dtype: int64\n",
      "---Value count for race\n",
      " White                 13946\n",
      " Black                  1561\n",
      " Asian-Pac-Islander      480\n",
      " Amer-Indian-Eskimo      159\n",
      " Other                   135\n",
      "Name: race, dtype: int64\n",
      "---Value count for income\n",
      "<=50K    12435\n",
      ">50K      3846\n",
      "Name: income, dtype: int64\n",
      "---Value count for education\n",
      " HS-grad         5283\n",
      " Some-college    3587\n",
      " Bachelors       2670\n",
      " Masters          934\n",
      " Assoc-voc        679\n",
      "11th              637\n",
      " Assoc-acdm       534\n",
      "10th              456\n",
      "7th-8th           309\n",
      " Prof-school      258\n",
      "9th               242\n",
      "12th              224\n",
      " Doctorate        181\n",
      "5th-6th           176\n",
      "1st-4th            79\n",
      " Preschool         32\n",
      "Name: education, dtype: int64\n",
      "---Value count for sex\n",
      " Male      10860\n",
      " Female     5421\n",
      "Name: sex, dtype: int64\n",
      "---Value count for native-country\n",
      " United-States                 14662\n",
      " Mexico                          308\n",
      " ?                               274\n",
      " Philippines                      97\n",
      " Puerto-Rico                      70\n",
      " Germany                          69\n",
      " Canada                           61\n",
      " India                            51\n",
      " El-Salvador                      49\n",
      " China                            47\n",
      " Cuba                             43\n",
      " England                          37\n",
      " South                            35\n",
      " Dominican-Republic               33\n",
      " Italy                            32\n",
      " Haiti                            31\n",
      " Japan                            30\n",
      " Portugal                         30\n",
      " Poland                           27\n",
      " Columbia                         26\n",
      " Jamaica                          25\n",
      " Guatemala                        24\n",
      " Greece                           20\n",
      " Vietnam                          19\n",
      " Ecuador                          17\n",
      " Iran                             16\n",
      " Peru                             15\n",
      " Nicaragua                        15\n",
      " Taiwan                           14\n",
      " Ireland                          13\n",
      " Thailand                         12\n",
      " Hong                             10\n",
      " Scotland                          9\n",
      " Cambodia                          9\n",
      " Outlying-US(Guam-USVI-etc)        9\n",
      " France                            9\n",
      " Trinadad&Tobago                   8\n",
      " Honduras                          7\n",
      " Yugoslavia                        7\n",
      " Hungary                           6\n",
      " Laos                              5\n",
      "Name: native-country, dtype: int64\n",
      "---Value count for occupation\n",
      " Prof-specialty       2032\n",
      " Exec-managerial      2020\n",
      " Craft-repair         2013\n",
      " Sales                1854\n",
      " Adm-clerical         1841\n",
      " Other-service        1628\n",
      " Machine-op-inspct    1020\n",
      " ?                     966\n",
      " Transport-moving      758\n",
      " Handlers-cleaners     702\n",
      " Tech-support          518\n",
      " Farming-fishing       496\n",
      " Protective-serv       334\n",
      " Priv-house-serv        93\n",
      " Armed-Forces            6\n",
      "Name: occupation, dtype: int64\n",
      "---Value count for marital-status\n",
      " Married-civ-spouse       7403\n",
      " Never-married            5434\n",
      " Divorced                 2190\n",
      " Widowed                   525\n",
      " Separated                 505\n",
      " Married-spouse-absent     210\n",
      " Married-AF-spouse          14\n",
      "Name: marital-status, dtype: int64\n",
      "---Value count for relationship\n",
      " Husband           6523\n",
      " Not-in-family     4278\n",
      " Own-child         2513\n",
      " Unmarried         1679\n",
      " Wife               763\n",
      " Other-relative     525\n",
      "Name: relationship, dtype: int64\n",
      "Training data shape: (32561, 15)\n",
      "Test data shape: (16281, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data summary statistics\")\n",
    "print(train.describe())\n",
    "for acol in cat_col:\n",
    "    print(\"---Value count for\", acol)\n",
    "    print(train[acol].value_counts())\n",
    "\n",
    "print(\"\\n\\n Test data summary statistics\")    \n",
    "print(test.describe())\n",
    "for acol in cat_col:\n",
    "    print(\"---Value count for\", acol)\n",
    "    print(test[acol].value_counts())\n",
    "    \n",
    "    \n",
    "print(\"Training data shape:\", train.shape)\n",
    "print(\"Test data shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing one-hot encoding...\n",
      "dealing with column workclass\n",
      "dealing with column race\n",
      "dealing with column education\n",
      "dealing with column sex\n",
      "dealing with column native-country\n",
      "dealing with column occupation\n",
      "dealing with column marital-status\n",
      "dealing with column relationship\n",
      "Remove these columns: ['workclass_ Never-worked' 'native-country_ Holand-Netherlands'\n",
      " 'occupation_ Armed-Forces']\n",
      "The shape of x_train (32561, 105)\n",
      "The shape of y_train (32561,)\n",
      "The shape of x_test (16281, 105)\n",
      "The shape of y_test (16281,)\n"
     ]
    }
   ],
   "source": [
    "train_y0 = train['income']\n",
    "test_y0 = test['income']\n",
    "\n",
    "train_x0 = train.loc[:, train.columns != 'income']\n",
    "test_x0 = test.loc[:, test.columns != 'income']\n",
    "\n",
    "yencoder = OneHotEncoder()\n",
    "yencoder.fit(train_y0.values.reshape(-1, 1))\n",
    "\n",
    "#the lable variables are ready\n",
    "y_train = yencoder.transform(train_y0.values.reshape(-1, 1)).toarray()[:, 1]\n",
    "y_test = yencoder.transform(test_y0.values.reshape(-1, 1)).toarray()[:, 1]\n",
    "\n",
    "\n",
    "xstd = StandardScaler()\n",
    "xstd.fit(train_x0[num_col].values.astype('float64'))\n",
    "#keep a copy of feature names\n",
    "feat_names = list(num_col.copy())\n",
    "x_train = xstd.transform(train_x0[num_col].values.astype('float64'))\n",
    "x_test = xstd.transform(test_x0[num_col].values.astype('float64'))\n",
    "\n",
    "#remember to remove income\n",
    "print(\"Doing one-hot encoding...\")\n",
    "for acol in set(cat_col) - set(['income']):\n",
    "    print(\"dealing with column\", acol)\n",
    "    tmp1 = train_x0[acol].values.reshape(-1, 1)\n",
    "    #establish code book using a threshold\n",
    "    cc = train_x0[acol].value_counts()\n",
    "    #frequency > 5\n",
    "    #ind1 = cc > 5\n",
    "    #codebook = cc[ind1].index.to_list()\n",
    "\n",
    "    xenc = OneHotEncoder(sparse = False)\n",
    "    xenc.fit(tmp1)\n",
    "    thisnames = acol + \"_\" + xenc.categories_[0]\n",
    "    xfeat_train = xenc.transform(tmp1)\n",
    "    xfeat_test = xenc.transform(test_x0[acol].values.reshape(-1, 1))\n",
    "\n",
    "    x_train = np.hstack((x_train, xfeat_train))\n",
    "    x_test = np.hstack((x_test, xfeat_test))\n",
    "    feat_names.extend(thisnames)\n",
    "\n",
    "#check the minimal frequency requirement\n",
    "min_freq = 10\n",
    "col_count = np.sum(x_train, axis = 0)\n",
    "ind_remove = col_count < min_freq\n",
    "#do not remove numerical-valued columns\n",
    "ind_remove[0:len(num_col)] = False\n",
    "\n",
    "feat_names2 = np.array(feat_names)\n",
    "print(\"Remove these columns:\", feat_names2[ind_remove])\n",
    "\n",
    "keepcol = np.logical_not(ind_remove)\n",
    "x_train = x_train[:, keepcol]\n",
    "x_test = x_test[:, keepcol]\n",
    "feat_names_final = feat_names2[keepcol]\n",
    "\n",
    "print(\"The shape of x_train\", x_train.shape)\n",
    "print(\"The shape of y_train\", y_train.shape)\n",
    "print(\"The shape of x_test\", x_test.shape)\n",
    "print(\"The shape of y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct mylogistic_l2 class\n",
    "+ Q2.2 Derive the gradient and hession matrix for the new E(w).\n",
    "+ Q2.3 Create your own mylogistic_l2 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogistic_l2:\n",
    "    def __init__(self, reg_vec, max_iter, tol, add_intercept):\n",
    "        \"\"\"reg_vec: the regularization coefficient vector\n",
    "           max_iter: maximum number of iteration to run for the Newton method\n",
    "           tol: tolerance for the objective function\n",
    "           add_intercept: whether to add intercept (a column of ones) at last column of the feature matrix\"\"\"\n",
    "        self.reg_vec = reg_vec\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.add_intercept = add_intercept\n",
    "        \n",
    "    def fit(self, x, y, verbal = False):\n",
    "        if self.add_intercept:\n",
    "            x = np.c_[x, np.ones(x.shape[0])]\n",
    "        \n",
    "        #initial value for w, using ridge regression solution\n",
    "        inv_xtx = np.linalg.pinv(np.matmul(np.transpose(x), x) + np.diag(self.reg_vec))\n",
    "        w = np.matmul(inv_xtx, np.matmul(np.transpose(x), y))\n",
    "\n",
    "        obj_value = np.inf\n",
    "        best_w = None\n",
    "        tol = self.tol\n",
    "        for iter in range(self.max_iter):\n",
    "            if verbal: print(\"----Iteration\", iter)\n",
    "                \n",
    "            y0 = 1 / (1 + np.exp(- np.matmul(x, w)))\n",
    "            #gradient\n",
    "            grad = self.reg_vec * w + np.matmul(np.transpose(x), y0 - y)\n",
    "            R = np.diag(y0 * (1-y0))\n",
    "            #hession\n",
    "            hess = np.diag(self.reg_vec) + np.matmul(np.transpose(x), np.matmul(R, x))\n",
    "            \n",
    "            #newton update\n",
    "            w = w - np.matmul(np.linalg.inv(hess), grad)\n",
    "            y0 = 1 / (1 + np.exp(- np.matmul(x, w)))\n",
    "\n",
    "            obj_value_new = np.sum(np.square(w) * self.reg_vec) * 0.5 - np.sum(y * np.log(y0) + (1-y) * np.log(1-y0))\n",
    "            if verbal: \n",
    "                print(\"    obj_function value=\", obj_value_new)\n",
    "                print(\"    w[0:5] = \", w[0:5])\n",
    "                print(\"    w[10:15] = \", w[10:15])\n",
    "                print(\"    w[-1] = \", w[-1])\n",
    "                \n",
    "            if obj_value_new < obj_value:\n",
    "                best_w = w\n",
    "\n",
    "            if (obj_value - obj_value_new) < tol:\n",
    "                self.niter = iter + 1\n",
    "                break\n",
    "            obj_value = obj_value_new\n",
    "            \n",
    "            self.best_w = best_w\n",
    "            self.obj_value = obj_value\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"doing prediction\"\"\"\n",
    "        if self.add_intercept:\n",
    "            x = np.c_[x, np.ones(x.shape[0])]\n",
    "        ypredprob = 1 / (1 + np.exp(- np.matmul(x, self.best_w)))\n",
    "        ypred = (ypredprob >= 0.5).astype('float64')\n",
    "        return(ypred)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Case 1: lambda = 1 for all coefficients¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of iteration = 7\n",
      "obj_function value= 10433.611065257732\n",
      "w[0:5] (numerical-valued coef) =  [2.19133161 0.36303875 0.34229569 0.07320921 0.25629539]\n",
      "w[10:15] (binary-valued coef) =  [ 0.11734477 -0.53163682 -0.3299868  -0.20622546 -0.43847755]\n",
      "w[-1] (incercept) =  -1.193220980731002\n",
      "accuracy for case 1: 0.8530188563356059\n"
     ]
    }
   ],
   "source": [
    "#### Case 1: \n",
    "lambda_vec = np.ones(x_train.shape[1] + 1) * 10\n",
    "mlr2 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 100, tol = 1e-5, add_intercept = True)\n",
    "mlr2.fit(x_train, y_train, verbal = False)\n",
    "\n",
    "print(\"num of iteration =\", mlr2.niter)\n",
    "print(\"obj_function value=\", mlr2.obj_value)\n",
    "print(\"w[0:5] (numerical-valued coef) = \", mlr2.best_w[0:5])\n",
    "print(\"w[10:15] (binary-valued coef) = \", mlr2.best_w[10:15])\n",
    "print(\"w[-1] (incercept) = \", mlr2.best_w[-1])\n",
    "\n",
    "pred1 = mlr2.predict(x_test)\n",
    "\n",
    "ncorrect = np.sum(y_test == pred1)\n",
    "accuracy1 = ncorrect / y_test.shape[0]\n",
    "print(\"accuracy for case 1:\", accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Case 2: lambda = 1 for all but the intercept, no regularization for incercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of iteration = 7\n",
      "obj_function value= 10416.059842212042\n",
      "w[0:5] (numerical-valued coef) =  [2.18855182 0.36394344 0.34518112 0.07351644 0.25647551]\n",
      "w[10:15] (binary-valued coef) =  [ 0.2868292  -0.35565475 -0.16082062 -0.18409595 -0.24623374]\n",
      "w[-1] (incercept) =  -2.942716176060639\n",
      "accuracy for case 2: 0.8532031202014618\n"
     ]
    }
   ],
   "source": [
    "#### Case 2\n",
    "lambda_vec = np.ones(x_train.shape[1] + 1) * 10\n",
    "lambda_vec[-1] = 0.0\n",
    "mlr2 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 100, tol = 1e-5, add_intercept = True)\n",
    "mlr2.fit(x_train, y_train, verbal = False)\n",
    "\n",
    "print(\"num of iteration =\", mlr2.niter)\n",
    "print(\"obj_function value=\", mlr2.obj_value)\n",
    "print(\"w[0:5] (numerical-valued coef) = \", mlr2.best_w[0:5])\n",
    "print(\"w[10:15] (binary-valued coef) = \", mlr2.best_w[10:15])\n",
    "print(\"w[-1] (incercept) = \", mlr2.best_w[-1])\n",
    "\n",
    "pred2 = mlr2.predict(x_test)\n",
    "\n",
    "ncorrect = np.sum(y_test == pred2)\n",
    "accuracy2 = ncorrect / y_test.shape[0]\n",
    "print(\"accuracy for case 2:\", accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for incercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of iteration = 7\n",
      "obj_function value= 10469.90445697634\n",
      "w[0:5] (numerical-valued coef) =  [2.18825202 0.36243945 0.3420522  0.07287258 0.25599129]\n",
      "w[10:15] (binary-valued coef) =  [ 0.11133215 -0.53193164 -0.32000688 -0.14475314 -0.38754967]\n",
      "w[-1] (incercept) =  -1.1469931181852007\n",
      "accuracy for case 3: 0.8530188563356059\n"
     ]
    }
   ],
   "source": [
    "#### Case 3\n",
    "lambda_vec = np.ones(x_train.shape[1] + 1) * 10\n",
    "lambda_vec[-1] = 0.0\n",
    "lambda_vec[len(num_col):] = 15\n",
    "mlr2 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 100, tol = 1e-5, add_intercept = True)\n",
    "mlr2.fit(x_train, y_train, verbal = False)\n",
    "\n",
    "print(\"num of iteration =\", mlr2.niter)\n",
    "print(\"obj_function value=\", mlr2.obj_value)\n",
    "print(\"w[0:5] (numerical-valued coef) = \", mlr2.best_w[0:5])\n",
    "print(\"w[10:15] (binary-valued coef) = \", mlr2.best_w[10:15])\n",
    "print(\"w[-1] (incercept) = \", mlr2.best_w[-1])\n",
    "\n",
    "pred3 = mlr2.predict(x_test)\n",
    "\n",
    "ncorrect = np.sum(y_test == pred3)\n",
    "accuracy3 = ncorrect / y_test.shape[0]\n",
    "print(\"accuracy for case 3:\", accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4 Search for the best hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.5 Use sklearn.linear_model.LogisticRegression to train and test the model (including hyper-parameter tuning). Compare the estimated parameters and test accuracy with those from your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7924302788844622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(adult_X_train, adult_Y_train)\n",
    "x = classifier.predict(adult_X_test)\n",
    "corrects = np.sum(x == adult_Y_test)\n",
    "print(\"accuracy: \", corrects/len(adult_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.847875166002656\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "classifier = LogisticRegression(solver='newton-cg', max_iter = 1000, tol = 1e-5, fit_intercept = True)\n",
    "classifier.fit(adult_X_train, adult_Y_train)\n",
    "x = classifier.predict(adult_X_test)\n",
    "corrects = np.sum(x == adult_Y_test)\n",
    "print(\"accuracy: \", corrects/len(adult_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
